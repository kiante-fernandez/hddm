import kabuki
import hddm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import glob
import os

include = ["sa", "st", "sv"]
params = hddm.generate.gen_rand_params(include=include) #sometime this generates values that throw an error (particularly when include has sv)

params = {'sv': 0,'sa':.1, 'st': 0, 'z': 0.5, 'v': 0.325, 't': 0.23, 'a': 1.5}

sim,sim_params = hddm.generate.gen_rand_data(params, size = 1000)

#SIMULATION ACROSS RANGE OF PARAMETERS GENERATED BY R SCRIPT
path = "sa_simulations"
rm_fil = 'sa_simulations/sa_simulations_condition-'
all_files = glob.glob(os.path.join(path , "*.csv"))
# filename = all_files[8]
for filename in all_files:
    #load in each dataframe
    df = pd.read_csv(filename, index_col=None, header=0)
    
    #for each subject simulate the parameters
    # because ndt will just shift the rt distributions, we hold in constant. 
    res_li = []
    for subject_i in range(np.max(df.subj_idx)):
        #get the condition information from file name
        col_coding_temp = filename.replace(rm_fil, '')

        params = {'sv': 0,'sa': df.sa[subject_i], 'st': 0, 'z': 0.5, 'v': df.v[subject_i], 't': 0.4, 'a': df.a[subject_i]}
        sim,sim_params = hddm.generate.gen_rand_data(params, size = 500)
        #add col with condition information
        sim['instruction'] = col_coding_temp.replace('_difficulty', '')[0]
        sim['difficulty'] = col_coding_temp.replace('accuracy_difficulty-', '').replace('speed_difficulty-','').replace('.csv','')[0]
        sim['sa'] = df.sa[subject_i]
        sim['subj_idx'] = subject_i + 1

        res_li.append(sim)
        
    frame = pd.concat(res_li, axis=0, ignore_index=True)
    file_temp_name = 'sa_simulations/sa_sim_res/' + filename.replace('sa_simulations/', '')
    frame.to_csv(file_temp_name)


sim_list = [sim1, sim2, sim3, sim4, sim5]
fig = plt.figure()
ax = fig.add_subplot(111, xlabel='RT', ylabel='count', title='RT distributions ')
for i in range(5):
    sim = sim_list[i]
    data = hddm.utils.flip_errors(sim)
    # data.rt.hist(bins=50, histtype='step', ax=ax)
    # sns.kdeplot(np.array(data.rt), bw=0.5)
    sns.distplot(data.rt)

plt.show()

hddm.model_config.model_config["full_ddm"]

include = ["sa", "st", "sv"]
params = hddm.generate.gen_rand_params(include=include) 
#test some values
params = {'sv': 0.09,'sa':.65, 'st': 0.3, 'z': 0.5, 'v': 0.425, 't': 0.23, 'a': 1.5}

sim,sim_params = hddm.generate.gen_rand_data(params, size = 1000) #note adding subjects makes the model really mad! 
model_0 = hddm.HDDM(sim, include = ("sa","sv", "st"))
# find a good starting point which helps with the convergence.
model_0.find_starting_values()
# start drawing 2000 samples and discarding 20 as burn-in (usually you want to have a longer burn-in period)
model_0.sample(10000, burn=5000)


stats = model_0.gen_stats()
stats
sim_params
# params = model_0.optimize('chisquare')
model_0.plot_posteriors(['a','v','sv','sa'])
plt.show()

hddm.plotting.plot_posterior_pair(model_0, save = False, figsize = (5, 5))
#seems like the recovery for sa is hard because of it's relation with sv
mle_est = model_0.optimize(method = "ML")
mle_est
sim_params
#notes 
# so generate data with 
sim,sim_params = hddm.generate.gen_rand_data(params, size = 1000) #note adding subjects makes the model really mad! 


def simulate_saDDM(params):
    #simulate five data sets with varying sa
    params1 = {'sv': params['sv'], 'sa': 0, 'st': params['st'], 'z': 0.5, 'v': params['v'], 't': params['t'], 'a': params['a']}
    params2 = {'sv': params['sv'], 'sa': 0.001, 'st': params['st'], 'z': 0.5, 'v': params['v'], 't': params['t'], 'a': params['a']}
    params3 = {'sv': params['sv'], 'sa': 0.01, 'st': params['st'], 'z': 0.5, 'v': params['v'], 't': params['t'], 'a': params['a']}
    params4 = {'sv': params['sv'], 'sa': 0.1, 'st': params['st'], 'z': 0.5, 'v': params['v'], 't': params['t'], 'a': params['a']}
    params5 = {'sv': params['sv'], 'sa': 1, 'st': params['st'], 'z': 0.5, 'v': params['v'], 't': params['t'], 'a': params['a']}

    sim1,sim_params1 = hddm.generate.gen_rand_data(params1, size = 1000)
    sim2,sim_params2 = hddm.generate.gen_rand_data(params2, size = 1000)
    sim3,sim_params3 = hddm.generate.gen_rand_data(params3, size = 1000)
    sim4,sim_params4 = hddm.generate.gen_rand_data(params4, size = 1000)
    sim5,sim_params5 = hddm.generate.gen_rand_data(params5, size = 1000, method = "cdf_py")

    sim1['sa'] = 0
    sim2['sa'] = 0.001
    sim3['sa'] = 0.01
    sim4['sa'] = 0.1
    sim5['sa'] = 1

    sims = pd.concat((sim1,sim2,sim3,sim4,sim5))
    
    return sims


#Increase a by 50%
params_ah = {'sv': 0, 'st': 0, 'z': 0.5, 'v': 0.65, 't': 0.23, 'a': 2.25}
sims1 = simulate_saDDM(params_ah)
sims1.to_csv("sa_simulations_high_a.csv")
#Decrease a by 50%
params_al = {'sv': 0, 'st': 0, 'z': 0.5, 'v': 0.65, 't': 0.23, 'a': 0.75}
sims2 = simulate_saDDM(params_al)
sims2.to_csv("sa_simulations_low_a.csv")
#Increase v by 50%
params_vh = {'sv': 0, 'st': 0, 'z': 0.5, 'v': 0.975, 't': 0.23, 'a': 1.5}
sims3 = simulate_saDDM(params_vh)
sims3.to_csv("sa_simulations_high_v.csv")
#Decrease v by 50%
params_vl = {'sv': 0, 'st': 0, 'z': 0.5, 'v': 0.325, 't': 0.23, 'a': 1.5}
sims4 = simulate_saDDM(params_vl)
sims4.to_csv("sa_simulations_low_v.csv")



# hddm.generate.gen_rts(method="cdf", **params).rt.values

# rts = np.asarray(sim["rt"])

# # rt = 1.5
# v = params['v']
# a = params['a']
# t = params['t']
# sa = params['sa']
# z = 0.5
# err = 10 ** (round(np.random.rand() * -10))
# st = params['st']
# sv = params['sv']

# python_wfpt = hddm.wfpt.full_pdf(-rt, v, sv, a, z, sa, t, st, err, n_sa=5)

# p = hddm.wfpt.pdf_array(rts,
#             params["v"],
#             params["sv"],
#             params["a"],
#             params["z"],
#             params["sa"],
#             params["t"],
#             params["st"],
#             1e-6,
#             logp=True,
#         )

# ar_nan = np.where(np.isinf(p))
# print (ar_nan)
# rts[ar_nan]

# summed_logp = np.sum(p)

# summed_logp_like = hddm.wfpt.wiener_like(
#             rts,
#             params["v"],
#             params["sv"],
#             params["a"],
#             params["z"],
#             params["sa"],
#             params["t"],
#             params["st"],
#             1e-4,
#         )

# my_res = hddm.wfpt.full_pdf(
#         rt, v=v, sv=sv, a=a, z=z, sa=sa, t=t, st=st, err=err, n_st=5, n_sa=5, use_adaptive=0,)
            
            


np.random.seed(123)

for tests in range(2):
        sv = rand() * 0.4 + 0.1
        v = (rand() - 0.5) * 4
        st = rand() * 0.3
        t = rand() * 0.5 + (st / 2)
        a = 1.5 + rand() 
        + sz / 2
        err = 10**-8
        sa = rand() * 0.3
        z = 0.5 * rand() 
        func = lambda x: np.exp(
            hddm.wfpt.wiener_like(np.array([x]), v, sv, a, z, sz, t, st, err)
        )
        integ, error = sp.integrate.quad(func, a=-5, b=5)

        np.testing.assert_almost_equal(integ, 1, 2)

a = 0.641422303761735
sa = 0.75
(a-sa/2.<0)

# if : return 0


# def gen_rts_from_simulated_drift(params, samples=1000, dt=1e-4, intra_sv=1.0):
#     """Returns simulated RTs from simulating the whole drift-process.

#     :Arguments:
#         params : dict
#             Parameter names and values.

#     :Optional:
#         samlpes : int
#             How many samples to generate.
#         dt : float
#             How many steps/sec.
#         intra_sv : float
#             Intra-trial variability.

#     :SeeAlso:
#         gen_rts
#     """

#     from numpy.random import rand

#     if samples is None:
#         samples = 1
#     nn = 1000
#     v = params["v"]
#     a = params["a"] 
    
#     if "v_switch" in params:
#         switch = True
#         t_switch = params["t_switch"] / dt
#         # Hack so that we will always step into a switch
#         nn = int(round(t_switch))
#     else:
#         switch = False

#     # create delay
#     if "st" in params:
#         start_delay = (
#             uniform.rvs(loc=params["t"], scale=params["st"], size=samples)
#             - params["st"] / 2.0
#         )
#     else:
#         start_delay = np.ones(samples) * params["t"]

#     # create boundary separation
#     if "sa" in params:
#         a = (
#             uniform.rvs(loc=params["a"], scale=params["sa"], size=samples)
#             - params["sa"] / 2.0
#         )
#     else:
#         a = np.array([params["a"]] * samples)

#     # create starting_points
#     starting_points = np.ones(samples) * params["z"] * a

#     rts = np.empty(samples)
#     step_size = np.sqrt(dt) * intra_sv
#     drifts = []

#     for i_sample in range(samples):
#         drift = np.array([])
#         crossed = False
#         iter = 0
#         y_0 = starting_points[i_sample]
#         # drifting...
#         if "sv" in params and params["sv"] != 0:
#             drift_rate = norm.rvs(v, params["sv"])
#         else:
#             drift_rate = v

#         if "v_switch" in params:
#             if "V_switch" in params and params["V_switch"] != 0:
#                 drift_rate_switch = norm.rvs(params["v_switch"], params["V_switch"])
#             else:
#                 drift_rate_switch = params["v_switch"]

#         prob_up = 0.5 * (1 + np.sqrt(dt) / intra_sv * drift_rate)

#         while not crossed:
#             # Generate nn steps
#             iter += 1
#             if iter == 2 and switch:
#                 prob_up = 0.5 * (1 + np.sqrt(dt) / intra_sv * drift_rate_switch)
#             position = ((rand(nn) < prob_up) * 2 - 1) * step_size
#             position[0] += y_0
#             position = np.cumsum(position)
#             # Find boundary crossings
#             cross_idx = np.where((position < 0) | (position > a[i_sample]))[0]

#             drift = np.concatenate((drift, position))
#             if cross_idx.shape[0] > 0:
#                 crossed = True
#             else:
#                 # If not crossed, set last position as starting point
#                 # for next nn steps to continue drift
#                 y_0 = position[-1]

#         # find the boundary interception
#         y2 = position[cross_idx[0]]
#         if cross_idx[0] != 0:
#             y1 = position[cross_idx[0] - 1]
#         else:
#             y1 = y_0
#         m = (y2 - y1) / dt  # slope
#         # y = m*x + b
#         b = y2 - m * ((iter - 1) * nn + cross_idx[0]) * dt  # intercept
#         if y2 < 0:
#             rt = (0 - b) / m
#         else:
#             rt = (a[i_sample] - b) / m
#         rts[i_sample] = (rt + start_delay[i_sample]) * np.sign(y2)

#         delay = start_delay[i_sample] / dt
#         drifts.append(
#             np.concatenate(
#                 (
#                     np.ones(int(delay)) * starting_points[i_sample],
#                     drift[: int(abs(rt) / dt)],
#                 )
#             )
#         )

#     return rts, drifts


# gen_rts_from_simulated_drift(params, samples = 1000)